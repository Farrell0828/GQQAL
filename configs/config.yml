# Dataset related arguments
dataset:
  train_csv_path: "data/train.csv"
  test_csv_path: "data/test.csv"
  sample_submission_csv_path: "data/sample_submission.csv"
  n_folds: 5

  # One of [ distilbert-base-uncased, 
  #          bert-base-uncased, bert-base-cased, 
  #          bert-large-uncased, bert-large-cased ]
  tokenizer_type: "distilbert-base-uncased"
  max_sequence_length:
    question_title: 30
    question_body: 512
    answer: 512
  
  
# Model related arguments
model:
  transformer_type: "distilbert-base-uncased" # same as tokenizer_type
  transformer_hidden_size: 768    # 768 for distilbert and bert base
# 1024 for bert large
  pool_method: "average"
  output_size: 30
  loss: mse


  # Training related arguments
solver:
  batch_size: 8
  n_epochs: 10
  initial_lr: 0.00005
  optimizer: 'AdamW'
  max_grad_norm: 1.0        # set to 0 if not clip gradient
  weight_decay: 0.01        # set to 0 if not use l2 regularization

  # Only work when weight_decay > 0
  no_decay:
    - 'bias'
    - 'LayerNorm.weight'

  # Learning rate schedule
  lr_schedule: 'warmup_linear'
  warmup_epochs: 1
  
  # Only work when lr_schedule is 'warmup_cosine_with_hard_restarts'
  cycles: 3
  