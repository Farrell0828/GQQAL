# Dataset related arguments
dataset:
  train_csv_path: data/train.csv
  test_csv_path: data/test.csv
  sample_submission_csv_path: data/sample_submission.csv
  n_folds: 5

  # One of [ distilbert-base-uncased, 
  #          bert-base-uncased, bert-base-cased, 
  #          bert-large-uncased, bert-large-cased ]
  tokenizer_type: distilbert-base-uncased
  split_to_sentences: true
  average_question_targets: true
  max_sequence_length:
    question_title: 30
    sentence: 100
  max_n_sentences: 30
  
  
# Model related arguments
model:
  fusioner: tqaa
  decoder: fc
  input_ndim: 3

  sentence_encoder:
    transformer_type: distilbert-base-uncased # same as tokenizer_type
    transformer_hidden_size: 768    # 768 for distilbert and bert base
                                    # 1024 for bert large
    pool_method: max


  tqaa_fusioner:
    hidden_size: 768
    attention_dropout: 0.1
    

  fc_decoder:
    input_size: 768
    hidden_size: 512
    n_hidden_layers: 1
    output_size: 30

  loss: bce       # 'mse' for mean square error loss
                  # 'bce' for binary cross entropy loss


# Training related arguments
solver:
  batch_size: 2
  n_epochs: 5
  initial_lr: 0.00004
  optimizer: AdamW
  max_grad_norm: 1.0        # set to 0 if not clip gradient
  weight_decay: 0.00        # set to 0 if not use l2 regularization

  # Only work when weight_decay > 0
  no_decay:
    - bias
    - LayerNorm.weight

  # Learning rate schedule
  lr_schedule: warmup_cosine
  warmup_epochs: 1
  
  # Only work when lr_schedule is warmup_cosine_with_hard_restarts
  cycles: 3
  